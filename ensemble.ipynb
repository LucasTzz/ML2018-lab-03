{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import math\n",
    "\n",
    "class AdaBoostClassifier:\n",
    "    '''A simple AdaBoost Classifier.'''\n",
    "\n",
    "    def __init__(self, weak_classifier, n_weakers_limit):\n",
    "        '''Initialize AdaBoostClassifier\n",
    "\n",
    "        Args:\n",
    "            weak_classifier: The class of weak classifier, which is recommend to be sklearn.tree.DecisionTreeClassifier.\n",
    "            n_weakers_limit: The maximum number of weak classifier the model can use.\n",
    "        '''\n",
    "        self.weak_classifier = weak_classifier\n",
    "        self.n_weakers_limit = n_weakers_limit\n",
    "        pass\n",
    "\n",
    "    def is_good_enough(self):\n",
    "        '''Optional'''\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''Build a boosted classifier from the training set (X, y).\n",
    "        Args:\n",
    "            X: An ndarray indicating the samples to be trained, which shape should be (n_samples,n_features).\n",
    "            y: An ndarray indicating the ground-truth labels correspond to X, which shape should be (n_samples,1).\n",
    "        '''\n",
    "        n_samples, n_features = X.shape\n",
    "        w = np.full(n_samples, 1/n_samples)\n",
    "#        index = [range(n_features)]\n",
    "        for epoch in range(self.n_weakers_limit):\n",
    "            self.weak_classifier = DecisionTreeClassifier()\n",
    "            self.weak_classifier = self.weak_classifier.fit(X=X, y=y, sample_weight=w)\n",
    "            w=w.reshape((-1,))\n",
    "            y = y.tolist()\n",
    "            predicts = self.predict(X)\n",
    "            predicts = predicts.tolist()\n",
    "            e=0\n",
    "            for i in range(n_samples):\n",
    "               if predicts[i] != y[i]:\n",
    "                   e += w[i]\n",
    "            e = max(e, 10**(-8))\n",
    "            a = 0.5 * math.log((1 - e) / e)\n",
    "            s = 0\n",
    "            for i in range(n_samples):\n",
    "                w[i] = w[i] * math.exp(-y[i] * a * predicts[i])\n",
    "                s += w[i]\n",
    "            for i in range(n_samples):\n",
    "                w[i] /= s\n",
    "            y = np.array(y)\n",
    "#        return self.weak_classifier\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    def predict_scores(self, X):\n",
    "        '''Calculate the weighted sum score of the whole base classifiers for given samples.\n",
    "\n",
    "        Args:\n",
    "            X: An ndarray indicating the samples to be predicted, which shape should be (n_samples,n_features).\n",
    "\n",
    "        Returns:\n",
    "            An one-dimension ndarray indicating the scores of differnt samples, which shape should be (n_samples,1).\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def predict(self, X, threshold=0):\n",
    "        '''Predict the catagories for geven samples.\n",
    "\n",
    "        Args:\n",
    "            X: An ndarray indicating the samples to be predicted, which shape should be (n_samples,n_features).\n",
    "            threshold: The demarcation number of deviding the samples into two parts.\n",
    "\n",
    "        Returns:\n",
    "            An ndarray consists of predicted labels, which shape should be (n_samples,1).\n",
    "        '''\n",
    "        predicts = self.weak_classifier.predict(X)\n",
    "        predicts = predicts.tolist()\n",
    "        for i in range(X.shape[0]):\n",
    "            if predicts[i] > threshold:\n",
    "                predicts[i] = 1\n",
    "            else:\n",
    "                predicts[i] = -1\n",
    "        predicts = np.array(predicts)\n",
    "        return predicts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def save(model, filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the NPD table...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn\n",
    "sys.path.append('/Users/taozizhuo/PycharmProjects/experiment/venv/ML2018-lab-03')\n",
    "from ensemble import AdaBoostClassifier\n",
    "import feature\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    def get_features(path):\n",
    "        image_paths = [os.path.join(path, f) for f in os.listdir(path)]\n",
    "#        ids = []\n",
    "        features = []\n",
    "\n",
    "        for image_path in image_paths:\n",
    "            img = cv2.imread(image_path)\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            img_reshape = cv2.resize(gray_img, (25, 25), interpolation=cv2.INTER_CUBIC)\n",
    "#            image_id = int(os.path.split(image_path)[-1].split(\".\")[0].split('_')[1])\n",
    "            image = feature.NPDFeature(img_reshape)\n",
    "            pre_features = feature.NPDFeature.extract(image)\n",
    "            pickle.dump(pre_features, open(\"save1.p\", \"wb\"))\n",
    "            face_feature = pickle.load(open(\"save1.p\", \"rb\"))\n",
    "            features.append(face_feature)\n",
    "#            ids.append(image_id)\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "    Faces = np.array(get_features('./datasets/original/face'))\n",
    "    Non_faces = np.array(get_features('./datasets/original/nonface'))\n",
    "    n_samples, n_features = Faces.shape\n",
    "    y_faces = []\n",
    "    y_non_faces = []\n",
    "    for i in range(n_samples):\n",
    "        y_faces.append(1)\n",
    "        y_non_faces.append(-1)\n",
    "    y_faces = np.array(y_faces).reshape((-1, 1))\n",
    "    y_non_faces = np.array(y_non_faces).reshape((-1, 1))\n",
    "#    faces_label = dict(zip(IDs, y_faces_features))\n",
    "#    non_faces_label = dict(zip(Non_IDs, y_non_faces_features))\n",
    "    Faces_train, Faces_val, y_faces_train, y_faces_val = train_test_split(Faces, y_faces, test_size=0.5)\n",
    "    Non_faces_train, Non_faces_val, y_non_faces_train, y_non_faces_val = train_test_split(Non_faces, y_non_faces,\n",
    "                                                                                          test_size=0.5)\n",
    "    X_train = np.concatenate((Faces_train, Non_faces_train), axis=0)\n",
    "    X_val = np.concatenate((Faces_val, Non_faces_val), axis=0)\n",
    "    y_train = np.concatenate((y_faces_train, y_non_faces_train), axis=0)\n",
    "    y_val = np.concatenate((y_faces_val, y_non_faces_val), axis=0)\n",
    "\n",
    "    X_train = np.column_stack((y_train, X_train))\n",
    "    np.random.shuffle(X_train)\n",
    "    y_train = X_train[:, 0]\n",
    "    X_train= np.delete(X_train, 0, axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    classifier = AdaBoostClassifier(sklearn.tree.DecisionTreeClassifier(), 10)\n",
    "    classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    predicts = classifier.predict(X_val)\n",
    "    report = sklearn.metrics.classification_report(y_true=y_val, y_pred=predicts, digits=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open('classifier_report.txt', 'w') as f:\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n          -1       0.83      0.89      0.86       250\\n           1       0.88      0.82      0.85       250\\n\\n   micro avg       0.86      0.86      0.86       500\\n   macro avg       0.86      0.86      0.86       500\\nweighted avg       0.86      0.86      0.86       500\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.88      0.83      0.85       250\n",
      "           1       0.84      0.88      0.86       250\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       500\n",
      "   macro avg       0.86      0.86      0.86       500\n",
      "weighted avg       0.86      0.86      0.86       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    f=open('classifier_report.txt', 'w')\n",
    "    f.write(report)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
